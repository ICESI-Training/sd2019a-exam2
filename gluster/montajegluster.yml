---
#- name: montaje gluster
  #hosts: all
  #become: true

  #  tasks:

          #    - name: assign content to /dev/sdb
          # shell:
                    # cmd: |
                    #       sfdisk /dev/sdb << EOF
                    #       ;
                    #       EOF
                            # ignore_errors: yes         

            #      - name: ejecutar comando  mkfs.xfs
            #shell: mkfs.xfs /dev/sdb1
            #ignore_errors: yes
          
            # - name: crear directorios  /gluster/data y  /dev/sdb
            #shell: mkdir -p /gluster/data /swarm/volumes
            # ignore_errors: yes

            # - name: montar /dev/sdb
            # shell: mount /dev/sdb1 /gluster/data/
            # ignore_errors: yes

            # - name: actualizar
            #yum:
                    # name: '*'
                    #  state: latest

                    #- name: montaje gluster en worker 2
                    #  hosts: 192.168.56.112
                    #  become: true

                    #  tasks:

                    #          - name: parar firewall
                    #          shell: systemctl stop firewalld

                    #         - name: deshabilitar firewall
                    #       shell: systemctl disable firewalld

        
          
- hosts: master
  become: yes
  tasks:
          - name: gluster add node1
            shell:
                    cmd: gluster peer probe 192.168.56.111
          - name: gluster add node2
            shell:
                    cmd: gluster peer probe 192.168.56.112
            ignore_errors: yes
          - name: gluster add node3
            shell:
                    cmd: gluster peer probe 192.168.56.113
          - name: create volume
            shell:
                    cmd: gluster volume create swarm-vols replica 4 192.168.56.110:/gluster/data 192.168.56.111:/gluster/data 192.168.56.112:/gluster/data 192.168.56.113:/gluster/data force     
            ignore_errors: yes
          
          - name: gluster start volume
            shell:
                    cmd: gluster volume start swarm-vols
            ignore_errors: yes
- hosts: all
  become: yes
  tasks:
          - name: mount swarm in all nodes
            shell:
                    cmd: mount.glusterfs localhost:/swarm-vols /swarm/volumes
            ignore_errors: yes
            #- hosts: master
            # become: yes
            # tasks:
            #- name: docker swarm init
            # shell: docker swarm init >> token.txt

            #- hosts: worker_1 worker_2 worker_3
  #become: yes
  #tasks:
          #- name: copiar archivo
          # shell: scp master@192.168.56.110 /home/token.txt

            #- name: join
            #  shell: docker swarm join --token $(cat token.txt | grep "\--token" | awk '{print $2}') 192.168.53.110:2377
        
           
