- hosts: all
  become: yes
  tasks:

          - name: configuration
            shell: |
                    echo "192.168.56.101 node0" >> /etc/hosts
                    echo "192.168.56.102 node1" >> /etc/hosts
                    echo "192.168.56.103 node2" >> /etc/hosts
                    echo "192.168.56.104 node3" >> /etc/hosts

          - name: install Docker
            yum:
                    name: docker
                    state: present
                    update_cache: true

  
          - name: start Docker
            service:
                    name: docker
                    state: started

          - name: Check if Docker compose is installed
            command: docker-compose --version
            register: docker_compose_check
            ignore_errors: yes
          
          - name: download and install Docker compose
            get_url:
                    url: https://github.com/docker/compose/releases/download/1.21.2/docker-compose-Linux-x86_64
                    dest: /usr/bin/docker-compose
                    mode: 0755
            when:
                    - docker_compose_check.msg is defined
                    - docker_compose_check.msg.find('No such file or directory') != -1
   
          - name: ensure net.bridge.bridge-nf-call-ip6tables is set to 1
            sysctl:
                    name: net.bridge.bridge-nf-call-ip6tables
                    value: 1
                    state: present
   
          - name: ensure net.bridge.bridge-nf-call-iptables is set to 1
            sysctl:
                    name: net.bridge.bridge-nf-call-iptables
                    value: 1
                    state: present
   
          - name: add Kubernetes' YUM repository
            yum_repository:
                    name: Kubernetes
                    description: Kubernetes YUM repository
                    baseurl: https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
                    gpgkey: https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
                    gpgcheck: yes
 
          - name: update
            yum:
                    name: '*'
                    state: latest
          
          - name: install kubelet
            yum:
                    name: kubelet-1.10.12
                    state: present
                    update_cache: true
   
          - name: install kubeadm
            yum:
                    name: kubeadm-1.10.12
                    state: present
   
          - name: start kubelet
            service:
                    name: kubelet
                    enabled: yes
                    state: started

          - name: update
            yum:
                    name: '*'
                    state: latest

          - name: install gluster dependencies
            yum:
                    name:
                            - wget
                            - centos-release-gluster
                            - epel-release

                    state: latest

          - name: install gluster
            yum:
                    name: glusterfs-server
                    state: latest

          - name: update
            yum:
                    name: '*'
                    state: latest

          - name: start gluster
            service:
                    name: glusterd
                    state: started
                    enabled: yes

          - name: update
            yum:
                    name: '*'
                    state: latest

          - name: install xfs
            yum:
                    name: xfsprogs

          - name: Volume partition
            parted:
                    device: /dev/sdb
                    number: 1
                    state: present

          - name: Creating a XFS fils system
            filesystem:
                    fstype: xfs
                    dev: /dev/sdb1

          - name: Directory
            shell: mkdir -p /gluster/data /swarm/volumes

          - name: Mount volume
            mount:
                    path: /dev/sdb1
                    src: /gluster/data
                    state: present
                    fstype: xfs

#          - name: change gluster permissions
#            file:
#                    path: /var/log/glusterfs/cli.log
#                    owner: vagrant
#                    mode: 755

          - name: change docker permissions
            file:
                    path: /var/run/docker.sock
                    owner: vagrant
                    mode: 755


- hosts: master
  become: yes
  tasks:
       
          - name: install kubectl
            yum:
                    name: kubectl-1.10.12
                    state: present
                    allow_downgrade: yes

- hosts: docker-manager
  become: yes
  become_user: vagrant
  become_method: sudo 
  tasks:

          - name: init swarm
            shell: docker swarm init --advertise-addr {{ ansible_eth1['ipv4']['address']  }}
            ignore_errors: yes

- hosts: docker-worker
  become: yes
  become_user: vagrant
  become_method: sudo
  vars:
          docker_swarm_manager_ip: "192.168.56.101"
  tasks:
          - name: generate token cluster
            shell: docker swarm join-token -q worker
            run_once: true
            register: docker_worker_token
            delegate_to: "{{ groups['docker-manager'][0]  }}"
            delegate_facts: true

          - name: little debug
            debug:
                    msg: "{{ docker_worker_token.stdout  }}"

          - name: join in the cluster
            shell: docker swarm join --token "{{ docker_worker_token.stdout  }}" "{{ docker_swarm_manager_ip  }}":2377
            run_once: true
            ignore_errors: yes

- hosts: master
  become: yes
  tasks:
          - name: gluster node_1
            shell: gluster peer probe node1

          - name: gluster node_2
            shell: gluster peer probe node2

          - name: gluster node_3
            shell: sudo gluster peer probe node3

          - name: gluster volume
            shell: gluster volume create swarm-vols replica 4 node0:/gluster/data node1:/gluster/data node2:/gluster/data node3:/gluster/data force

          - name: gluster start volume
            shell: gluster volume start swarm-vols
